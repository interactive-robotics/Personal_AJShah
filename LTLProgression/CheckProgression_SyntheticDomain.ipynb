{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', ['G', ['and', ['not', ['T0']], ['not', ['T1']], ['not', ['T2']], ['not', ['T3']], ['not', ['T4']]]], ['imp', ['P0'], ['F', ['W0']]], ['imp', ['P1'], ['F', ['W1']]], ['imp', ['P2'], ['F', ['W2']]], ['imp', ['P3'], ['F', ['W3']]], ['imp', ['P0'], ['U', ['not', ['W1']], ['W0']]], ['imp', ['P0'], ['U', ['not', ['W2']], ['W0']]], ['imp', ['P1'], ['U', ['not', ['W2']], ['W1']]]]\n"
     ]
    }
   ],
   "source": [
    "from FormulaTools import *\n",
    "from Progression import *\n",
    "from Constants import *\n",
    "from SpecificationMDPTools import *\n",
    "import numpy as np\n",
    "\n",
    "PathToDataFile = ''\n",
    "SampleSignal = ImportSampleData(PathToDataFile)\n",
    "\n",
    "\n",
    "Formulas, Prob = ImportCandidateFormulas()\n",
    "idx = np.argsort(Prob)[::-1]\n",
    "Formulas = np.array(Formulas)\n",
    "Prob = np.array(Prob)\n",
    "Formulas = Formulas[idx]\n",
    "Prob = Prob[idx]\n",
    "formula = Formulas[0]\n",
    "print(formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now first lets verify that the syntax of the sampled formulas is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 'AndOr')\n"
     ]
    }
   ],
   "source": [
    "print(VerifyFormulaSyntax(formula))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check that the vocabulary of the formula is a subset of the signal vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formula Vocabulary {'P3', 'W3', 'T2', 'P0', 'T0', 'W2', 'P1', 'T3', 'W1', 'T4', 'P2', 'W0', 'T1'}\n",
      "Signal predicates {'P3', 'W3', 'T2', 'P0', 'T0', 'W2', 'P1', 'W1', 'T3', 'T4', 'P2', 'W0', 'T1'}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print('Formula Vocabulary', GetVocabulary(formula, set()))\n",
    "print('Signal predicates', set(SampleSignal.keys()) - set(['length']))\n",
    "print(VerifyVocabulary(formula, SampleSignal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets generate a slice of the labeling propositions at the first time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'P3': True, 'W3': False, 'T2': False, 'P0': True, 'T0': False, 'W2': False, 'P1': True, 'W1': False, 'T3': False, 'T4': False, 'P2': True, 'W0': False, 'T1': False}\n"
     ]
    }
   ],
   "source": [
    "SignalSlice = GetTraceSlice(SampleSignal,0)\n",
    "print(SignalSlice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The formulas have conditional predicates that make their effect felt from the first time step and are not relevant in the dynamic part of the task, so lets progress the formula by a single time step before beginning the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', ['G', ['and', ['not', ['T0']], ['not', ['T1']], ['not', ['T2']], ['not', ['T3']], ['not', ['T4']]]], ['F', ['W0']], ['F', ['W1']], ['F', ['W2']], ['F', ['W3']], ['U', ['not', ['W1']], ['W0']], ['U', ['not', ['W2']], ['W0']], ['U', ['not', ['W2']], ['W1']]]\n"
     ]
    }
   ],
   "source": [
    "progressed_formula = ProgressSingleTimeStep(formula, SignalSlice)\n",
    "print(progressed_formula)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the progressed formula has a more limited vocabulary compared to the original formula and this is better for progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T0', 'T1', 'T2', 'T3', 'T4', 'W0', 'W1', 'W2', 'W3'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GetVocabulary(progressed_formula, set())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use `FindAllProgressions_single_formula` to evaluate all possible progressions of the given formula and a description of the graph of formulas given the truth assignments in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "progressions, edges = FindAllProgressions_single_formula(progressed_formula)\n",
    "states = dict([(v,k) for (k,v) in progressions.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as the LTL formulas were stored as nested lists, we have to convert them to json strings to use a dictionary to hash them. `progressions` is a dictionary where the keys represent all possible progressions of the original formula. The value stored at the key represents a unique integer index that can be used as a one-hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"and\", [\"G\", [\"and\", [\"not\", [\"T0\"]], [\"not\", [\"T1\"]], [\"not\", [\"T2\"]], [\"not\", [\"T3\"]], [\"not\", [\"T4\"]]]], [\"F\", [\"W0\"]], [\"F\", [\"W1\"]], [\"F\", [\"W2\"]], [\"F\", [\"W3\"]], [\"U\", [\"not\", [\"W1\"]], [\"W0\"]], [\"U\", [\"not\", [\"W2\"]], [\"W0\"]], [\"U\", [\"not\", [\"W2\"]], [\"W1\"]]]\n"
     ]
    }
   ],
   "source": [
    "key = list(progressions.keys())[0]\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`states` is the inverse map of the states to the LTL formula representing the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[0]==key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`edges` is a tuple structure that represents the graph of transitions between the LTL formulas and the truth value assignments that cause that transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The source state/formula is:\n",
      " ['and', ['G', ['and', ['not', ['T0']], ['not', ['T1']], ['not', ['T2']], ['not', ['T3']], ['not', ['T4']]]], ['F', ['W0']], ['F', ['W1']], ['F', ['W2']], ['F', ['W3']], ['U', ['not', ['W1']], ['W0']], ['U', ['not', ['W2']], ['W0']], ['U', ['not', ['W2']], ['W1']]]\n",
      "\n",
      "\n",
      "With truth assignment: \n",
      "[{'T0': False, 'T1': False, 'T2': False, 'T3': False, 'T4': False, 'W0': True, 'W1': False, 'W2': False, 'W3': True}]\n",
      "\n",
      "\n",
      "The resulting formula is: \n",
      " ['and', ['G', ['and', ['not', ['T0']], ['not', ['T1']], ['not', ['T2']], ['not', ['T3']], ['not', ['T4']]]], ['F', ['W1']], ['F', ['W2']], ['U', ['not', ['W2']], ['W1']]]\n"
     ]
    }
   ],
   "source": [
    "edge = edges[5]\n",
    "print(f'The source state/formula is:\\n {edge[0]}\\n\\n')\n",
    "print(f'With truth assignment: \\n{edge[2]}\\n\\n')\n",
    "print(f'The resulting formula is: \\n {edge[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both 'W1' and 'W2' were completed simultaneously\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the progression states are known, we can use `FindTerminalStates_single_formula` to identify the progressions that can serve as the end of a demonstration. The terminal states are when the formula is either progressed to `[True]` or to a cosafe formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[false]', '[\"G\", [\"and\", [\"not\", [\"T0\"]], [\"not\", [\"T1\"]], [\"not\", [\"T2\"]], [\"not\", [\"T3\"]], [\"not\", [\"T4\"]]]]']\n"
     ]
    }
   ],
   "source": [
    "terminal_states = FindTerminalStates_single_formula(progressions)\n",
    "print(terminal_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now lets consider an example from the dinner table domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "Data = json.load(open('DinnerTable_OutputDist_Sampler4_Custom_30.json','r'))\n",
    "formula = Data['support'][1]\n",
    "Signal = ImportBSIData('DinnerTableData.json')\n",
    "TraceSlice = GetTraceSlice(Signal, 0)\n",
    "progressed_formula = ProgressSingleTimeStep(formula, TraceSlice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the original formula contains 20 propositions and 3 orderings, that represents a pretty large set of formulas that can be progressed to including all possible evaluations of the conditionals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', ['G', ['not', ['T0']]], ['imp', ['P0'], ['F', ['W0']]], ['imp', ['P1'], ['F', ['W1']]], ['imp', ['P2'], ['F', ['W2']]], ['imp', ['P3'], ['F', ['W3']]], ['imp', ['P5'], ['F', ['W5']]], ['imp', ['P6'], ['F', ['W6']]], ['imp', ['P1'], ['U', ['not', ['W2']], ['W1']]], ['imp', ['P3'], ['U', ['not', ['W6']], ['W3']]], ['imp', ['P7'], ['U', ['not', ['W5']], ['W7']]]] :  0 \n",
      "\n",
      "[False] :  1 \n",
      "\n",
      "['G', ['not', ['T0']]] :  2 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W6']]] :  3 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W5']]] :  4 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W5']], ['U', ['not', ['W5']], ['W7']]] :  5 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W5']], ['F', ['W6']]] :  6 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W5']], ['W7']]] :  7 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W3']], ['F', ['W6']], ['U', ['not', ['W6']], ['W3']]] :  8 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W3']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W6']], ['W3']]] :  9 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W3']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  10 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W2']]] :  11 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W2']], ['F', ['W6']]] :  12 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W2']], ['F', ['W5']]] :  13 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W2']], ['F', ['W5']], ['U', ['not', ['W5']], ['W7']]] :  14 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W2']], ['F', ['W5']], ['F', ['W6']]] :  15 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W2']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W5']], ['W7']]] :  16 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W2']], ['F', ['W3']], ['F', ['W6']], ['U', ['not', ['W6']], ['W3']]] :  17 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W2']], ['F', ['W3']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W6']], ['W3']]] :  18 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W2']], ['F', ['W3']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  19 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W2']], ['U', ['not', ['W2']], ['W1']]] :  20 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W2']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']]] :  21 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W2']], ['F', ['W5']], ['U', ['not', ['W2']], ['W1']]] :  22 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W2']], ['F', ['W5']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W5']], ['W7']]] :  23 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W2']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']]] :  24 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W2']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W5']], ['W7']]] :  25 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W2']], ['F', ['W3']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']]] :  26 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W2']], ['F', ['W3']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']]] :  27 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W2']], ['F', ['W3']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  28 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']]] :  29 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W6']]] :  30 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W5']]] :  31 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W5']], ['U', ['not', ['W5']], ['W7']]] :  32 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W5']], ['F', ['W6']]] :  33 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W5']], ['W7']]] :  34 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W3']], ['F', ['W6']], ['U', ['not', ['W6']], ['W3']]] :  35 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W3']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W6']], ['W3']]] :  36 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W3']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  37 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W2']]] :  38 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W2']], ['F', ['W6']]] :  39 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W2']], ['F', ['W5']]] :  40 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W2']], ['F', ['W5']], ['U', ['not', ['W5']], ['W7']]] :  41 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W2']], ['F', ['W5']], ['F', ['W6']]] :  42 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W2']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W5']], ['W7']]] :  43 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W2']], ['F', ['W3']], ['F', ['W6']], ['U', ['not', ['W6']], ['W3']]] :  44 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W2']], ['F', ['W3']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W6']], ['W3']]] :  45 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W2']], ['F', ['W3']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  46 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W2']], ['U', ['not', ['W2']], ['W1']]] :  47 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W2']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']]] :  48 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W2']], ['F', ['W5']], ['U', ['not', ['W2']], ['W1']]] :  49 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W2']], ['F', ['W5']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W5']], ['W7']]] :  50 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W2']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']]] :  51 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W2']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W5']], ['W7']]] :  52 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W2']], ['F', ['W3']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']]] :  53 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W2']], ['F', ['W3']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']]] :  54 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W2']], ['F', ['W3']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  55 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W3']], ['U', ['not', ['W6']], ['W3']]] :  56 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W3']], ['F', ['W5']], ['U', ['not', ['W6']], ['W3']]] :  57 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W3']], ['F', ['W5']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  58 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W2']], ['F', ['W3']], ['U', ['not', ['W6']], ['W3']]] :  59 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W2']], ['F', ['W3']], ['F', ['W5']], ['U', ['not', ['W6']], ['W3']]] :  60 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W2']], ['F', ['W3']], ['F', ['W5']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  61 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W2']], ['F', ['W3']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']]] :  62 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W2']], ['F', ['W3']], ['F', ['W5']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']]] :  63 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W2']], ['F', ['W3']], ['F', ['W5']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  64 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W3']], ['U', ['not', ['W6']], ['W3']]] :  65 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W3']], ['F', ['W5']], ['U', ['not', ['W6']], ['W3']]] :  66 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W3']], ['F', ['W5']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  67 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W2']], ['F', ['W3']], ['U', ['not', ['W6']], ['W3']]] :  68 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W2']], ['F', ['W3']], ['F', ['W5']], ['U', ['not', ['W6']], ['W3']]] :  69 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W2']], ['F', ['W3']], ['F', ['W5']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  70 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W2']], ['F', ['W3']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']]] :  71 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W2']], ['F', ['W3']], ['F', ['W5']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']]] :  72 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W2']], ['F', ['W3']], ['F', ['W5']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  73 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['U', ['not', ['W5']], ['W7']]] :  74 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W6']], ['U', ['not', ['W5']], ['W7']]] :  75 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W3']], ['F', ['W6']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  76 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W2']], ['U', ['not', ['W5']], ['W7']]] :  77 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W2']], ['F', ['W6']], ['U', ['not', ['W5']], ['W7']]] :  78 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W2']], ['F', ['W3']], ['F', ['W6']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  79 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W2']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W5']], ['W7']]] :  80 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W2']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W5']], ['W7']]] :  81 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W2']], ['F', ['W3']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  82 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['U', ['not', ['W5']], ['W7']]] :  83 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W6']], ['U', ['not', ['W5']], ['W7']]] :  84 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W3']], ['F', ['W6']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  85 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W2']], ['U', ['not', ['W5']], ['W7']]] :  86 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W2']], ['F', ['W6']], ['U', ['not', ['W5']], ['W7']]] :  87 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W2']], ['F', ['W3']], ['F', ['W6']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  88 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W2']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W5']], ['W7']]] :  89 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W2']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W5']], ['W7']]] :  90 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W2']], ['F', ['W3']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  91 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W3']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  92 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W2']], ['F', ['W3']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  93 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W2']], ['F', ['W3']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  94 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W3']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  95 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W2']], ['F', ['W3']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  96 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W2']], ['F', ['W3']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  97 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['U', ['not', ['W2']], ['W1']]] :  98 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']]] :  99 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W5']], ['U', ['not', ['W2']], ['W1']]] :  100 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W5']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W5']], ['W7']]] :  101 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']]] :  102 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W5']], ['W7']]] :  103 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W3']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']]] :  104 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W3']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']]] :  105 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W3']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  106 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['U', ['not', ['W2']], ['W1']]] :  107 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']]] :  108 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W5']], ['U', ['not', ['W2']], ['W1']]] :  109 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W5']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W5']], ['W7']]] :  110 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']]] :  111 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W5']], ['W7']]] :  112 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W3']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']]] :  113 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W3']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']]] :  114 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W3']], ['F', ['W5']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  115 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W3']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']]] :  116 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W3']], ['F', ['W5']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']]] :  117 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W3']], ['F', ['W5']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  118 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W3']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']]] :  119 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W3']], ['F', ['W5']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']]] :  120 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W3']], ['F', ['W5']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  121 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W5']], ['W7']]] :  122 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W5']], ['W7']]] :  123 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W3']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  124 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W5']], ['W7']]] :  125 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W5']], ['W7']]] :  126 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W3']], ['F', ['W6']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  127 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W1']], ['F', ['W3']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  128 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W1']], ['F', ['W3']], ['U', ['not', ['W2']], ['W1']], ['U', ['not', ['W6']], ['W3']], ['U', ['not', ['W5']], ['W7']]] :  129 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Progressions, Edges = FindAllProgressions_single_formula(formula)\n",
    "for val in Progressions.keys():\n",
    "    print(json.loads(val), ': ', Progressions[val], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare this to what happens when we progress it by a single time step for a given scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W2']], ['F', ['W5']]] :  0 \n",
      "\n",
      "[False] :  1 \n",
      "\n",
      "['G', ['not', ['T0']]] :  2 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W5']]] :  3 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W2']]] :  4 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W2']], ['F', ['W5']]] :  5 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']]] :  6 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W5']]] :  7 \n",
      "\n",
      "['and', ['G', ['not', ['T0']]], ['F', ['W0']], ['F', ['W2']]] :  8 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Progressions_one_step, Edges_one_step = FindAllProgressions_single_formula(progressed_formula)\n",
    "for val in Progressions_one_step.keys():\n",
    "    print(json.loads(val), ': ', Progressions_one_step[val], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be a subset of all possible progressions of the original formula. Lets check that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "Set1 = set(Progressions.keys())\n",
    "Set2 = set(Progressions_one_step.keys())\n",
    "print(Set2.issubset(Set1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of Bayesian specification inference is not a single formula but a cateforical distribution over a collection of formulas. Thus the progression state is not a formula, but a collection of formulas consisting of different progression stage of each of the component formulas. Lets see how to get these progression states for a list of 150 top formulas in the synthetic domain dataset. Lets reload the dataset first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PathToDataFile = ''\n",
    "SampleSignal = ImportSampleData(PathToDataFile)\n",
    "TraceSlice = GetTraceSlice(SampleSignal,0)\n",
    "Formulas, Prob = ImportCandidateFormulas()\n",
    "idx = np.argsort(Prob)[::-1]\n",
    "Formulas = np.array(Formulas)[idx]\n",
    "Prob = np.array(Prob)[idx]\n",
    "ProgressedFormulas = np.array([ProgressSingleTimeStep(formula, TraceSlice) for formula in Formulas])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets find all possible progression states for MAP formula in the distribution. This should be the same as the case above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of progression states:  9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progression_states_speclist, edges_speclist = FindAllProgressions([ProgressedFormulas[0]])\n",
    "states_speclist = set([key[0] for key in progression_states_speclist.keys()])\n",
    "\n",
    "progression_states_singleformula, edges_singleformula = FindAllProgressions_single_formula(ProgressedFormulas[0])\n",
    "states_singleformula = set(progression_states_singleformula.keys())\n",
    "\n",
    "print('The number of progression states: ', len(progression_states_speclist))\n",
    "states_speclist == states_singleformula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has the same 9 progression states as the single formula. Now lets see what happens when we start including more formulas into the specification set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of progression states:  17\n"
     ]
    }
   ],
   "source": [
    "progression_states, edges = FindAllProgressions(ProgressedFormulas[0:5])\n",
    "print('The number of progression states: ', len(progression_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cumulative probability with 5 formulas is:  0.454066666667\n",
      "The number of progression states with 5 formulas is:  17 \n",
      "\n",
      "The cumulative probability with 10 formulas is:  0.541866666667\n",
      "The number of progression states with 10 formulas is:  25 \n",
      "\n",
      "The cumulative probability with 50 formulas is:  0.838866666667\n",
      "The number of progression states with 50 formulas is:  79 \n",
      "\n",
      "The cumulative probability with 100 formulas is:  0.949133333333\n",
      "The number of progression states with 100 formulas is:  135 \n",
      "\n",
      "The cumulative probability with 150 formulas is:  0.993266666667\n",
      "The number of progression states with 150 formulas is:  183 \n",
      "\n",
      "The cumulative probability with 178 formulas is:  1.0\n",
      "The number of progression states with 178 formulas is:  237 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [5, 10, 50, 100, 150,178]:\n",
    "    progression_states, edges = FindAllProgressions(ProgressedFormulas[0:i])\n",
    "    print(f'The cumulative probability with {i} formulas is: ', np.sum(Prob[0:i]))\n",
    "    print(f'The number of progression states with {i} formulas is: ', len(progression_states),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a few different formulas in support lets compare this by constructing the progression states naively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of naive progression states with 1 formulas is:  9\n",
      "The number of possible progression states with 1 formulas is:  9 \n",
      "\n",
      "The number of naive progression states with 2 formulas is:  63\n",
      "The number of possible progression states with 2 formulas is:  11 \n",
      "\n",
      "The number of naive progression states with 3 formulas is:  441\n",
      "The number of possible progression states with 3 formulas is:  11 \n",
      "\n",
      "The number of naive progression states with 4 formulas is:  3969\n",
      "The number of possible progression states with 4 formulas is:  11 \n",
      "\n",
      "The number of naive progression states with 5 formulas is:  27783\n",
      "The number of possible progression states with 5 formulas is:  17 \n",
      "\n",
      "The number of naive progression states with 6 formulas is:  194481\n",
      "The number of possible progression states with 6 formulas is:  17 \n",
      "\n",
      "The number of naive progression states with 7 formulas is:  1750329\n",
      "The number of possible progression states with 7 formulas is:  25 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Vals = [1,2,3,4,5,6,7]\n",
    "\n",
    "\n",
    "for nFormulas in Vals:\n",
    "    individual_progression_states = {}\n",
    "    # Do the naive state enumeration\n",
    "    for formula in ProgressedFormulas[0:nFormulas]:\n",
    "        individual_progression_states[json.dumps(formula)], _  = FindAllProgressions_single_formula(formula)\n",
    "\n",
    "    from itertools import product\n",
    "\n",
    "    progression_states_naive = list(product(*tuple([list(individual_progression_states[key].keys()) for key in individual_progression_states.keys()])))\n",
    "    print(f'The number of naive progression states with {nFormulas} formulas is: ', len(progression_states_naive))\n",
    "    \n",
    "    #Do the smart enumeration\n",
    "    progression_states, _ = FindAllProgressions(ProgressedFormulas[0:nFormulas])\n",
    "    print(f'The number of possible progression states with {nFormulas} formulas is: ', len(progression_states), '\\n')\n",
    "    #check if the list of progression states we compute is a subset of the ones computed naively (should be)\n",
    "\n",
    "    set(progression_states.keys()).issubset(set(progression_states_naive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly naively listing all possible progressions is a terrible idea. Our approach for a well constructed specification sets will settle on a reasonable number of progression states quite well. Lets also verify that this approach also works with the dinner table domain example. The dinner table formulas has fewer ordering constraint thus it has more possible progression states per formula. We start by loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of formulas in the posterior is:  25\n"
     ]
    }
   ],
   "source": [
    "Data = json.load(open('DinnerTable_OutputDist_Sampler4_Custom_30.json','r'))\n",
    "Formulas, Probs = Data['support'],Data['probs']\n",
    "#Sort the formulas as per the probabilities\n",
    "idx = np.argsort(Probs)[::-1]\n",
    "Formulas = np.array(Formulas)[idx]\n",
    "Probs = np.array(Probs)[idx]\n",
    "\n",
    "#Proposition data\n",
    "Signal = ImportBSIData('DinnerTableData.json')\n",
    "TraceSlice = GetTraceSlice(Signal, 0)\n",
    "Conditions = [key for key in TraceSlice.keys() if 'P' in key]\n",
    "for key in Conditions: TraceSlice[key] = True\n",
    "\n",
    "\n",
    "#Progress every formula by a single time step\n",
    "ProgressedFormulas = np.array([ProgressSingleTimeStep(formula, TraceSlice) for formula in Formulas])\n",
    "print('The number of formulas in the posterior is: ', len(ProgressedFormulas))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check if for a single formulas this gives the correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check with single progression\n",
    "progression_states_single_formula, edges_single_formula = FindAllProgressions_single_formula(ProgressedFormulas[0])\n",
    "Set1 = set(progression_states_single_formula)\n",
    "\n",
    "#Use set progression and verify that for a single formula the sets are equal\n",
    "progression_states_spec_list, edges_spec_list = FindAllProgressions([ProgressedFormulas[0]])\n",
    "Set2 = set([key[0] for key in progression_states_spec_list.keys()])\n",
    "\n",
    "Set1 == Set2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the progression states sizes for different formula sets in the specification list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cumulative probability with 1 formulas is:  0.276133333333\n",
      "The number of unique progression states with 1 formulas is:  193 \n",
      "\n",
      "The cumulative probability with 5 formulas is:  0.942533333333\n",
      "The number of unique progression states with 5 formulas is:  521 \n",
      "\n",
      "The cumulative probability with 10 formulas is:  0.971533333333\n",
      "The number of unique progression states with 10 formulas is:  1155 \n",
      "\n",
      "The cumulative probability with 15 formulas is:  0.9894\n",
      "The number of unique progression states with 15 formulas is:  1437 \n",
      "\n",
      "The cumulative probability with 20 formulas is:  0.997933333333\n",
      "The number of unique progression states with 20 formulas is:  2023 \n",
      "\n",
      "The cumulative probability with 25 formulas is:  1.0\n",
      "The number of unique progression states with 25 formulas is:  3024 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nFormulas = [1,5,10,15,20,25]\n",
    "\n",
    "for nForm in nFormulas:\n",
    "    print(f'The cumulative probability with {nForm} formulas is: ', np.sum(Probs[0:nForm]))\n",
    "    progression_states, edges = FindAllProgressions(ProgressedFormulas[0:nForm])\n",
    "    print(f'The number of unique progression states with {nForm} formulas is: ', len(progression_states),'\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can compare the sizes of the naive and the actual number of progression states for this too, however even with just 2 candidate formulas, we run out of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets determine the terminal states in both the synthetic domain and the dinner table domain with all the formulas in the posterior considered. Lets reload the dataset with the scoped names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding progression states for Synthetic Domain\n",
      "Number of unique progressions in Synthetic Domain:  237\n",
      "Number of edges in Synthetic Domain:  3190\n",
      "Number of terminal states in Synthetic Domain:  62 \n",
      "\n",
      "Finding progression states for Dinner Table Domain\n",
      "Number of unique progressions in Dinner Table Domain:  3024\n",
      "Number of edges in Dinner Table Domain:  26562\n",
      "Number of terminal states in Dinner Table Domain:  326 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "PathToDataFile = ''\n",
    "SampleSignal = ImportSampleData(PathToDataFile)\n",
    "TraceSlice = GetTraceSlice(SampleSignal,0)\n",
    "Formulas, Prob = ImportCandidateFormulas()\n",
    "idx = np.argsort(Prob)[::-1]\n",
    "Formulas_synth = np.array(Formulas)[idx]\n",
    "Probs_synth = np.array(Prob)[idx]\n",
    "ProgressedFormulas_synth = np.array([ProgressSingleTimeStep(formula, TraceSlice) for formula in Formulas_synth])\n",
    "\n",
    "Data = json.load(open('DinnerTable_OutputDist_Sampler4_Custom_30.json','r'))\n",
    "Formulas, Probs = Data['support'],Data['probs']\n",
    "#Sort the formulas as per the probabilities\n",
    "idx = np.argsort(Probs)[::-1]\n",
    "Formulas_dinner = np.array(Formulas)[idx]\n",
    "Probs_dinner = np.array(Probs)[idx]\n",
    "\n",
    "#Proposition data\n",
    "Signal = ImportBSIData('DinnerTableData.json')\n",
    "TraceSlice = GetTraceSlice(Signal, 0)\n",
    "Conditions = [key for key in TraceSlice.keys() if 'P' in key]\n",
    "for key in Conditions: TraceSlice[key] = True\n",
    "\n",
    "\n",
    "#Progress every formula by a single time step\n",
    "ProgressedFormulas_dinner = np.array([ProgressSingleTimeStep(formula, TraceSlice) for formula in Formulas_dinner])\n",
    "#print('The number of formulas in the posterior is: ', len(ProgressedFormulas_dinner))\n",
    "\n",
    "ProgressedFormulas = {'synth': ProgressedFormulas_synth, 'dinner': ProgressedFormulas_dinner}\n",
    "Probs = {'synth': Probs_synth, 'dinner': Probs_dinner}\n",
    "Formulas = {'synth': Formulas_synth, 'dinner': Formulas_dinner}\n",
    "\n",
    "progression_states = {}\n",
    "edges = {}\n",
    "terminal_states = {}\n",
    "Domain = {'synth': 'Synthetic Domain', 'dinner': 'Dinner Table Domain'}\n",
    "\n",
    "keys = ['synth', 'dinner']\n",
    "for key in keys:\n",
    "    domain = Domain[key]\n",
    "    print(f'Finding progression states for {domain}')\n",
    "    progression_states[key], edges[key] = FindAllProgressions(ProgressedFormulas[key])\n",
    "    terminal_states[key] = FindTerminalStates(progression_states[key])\n",
    "    print(f'Number of unique progressions in {domain}: ', len(progression_states[key]))\n",
    "    print(f'Number of edges in {domain}: ', len(edges[key]))\n",
    "    print(f'Number of terminal states in {domain}: ', len(terminal_states[key]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
