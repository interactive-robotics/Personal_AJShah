from DemoScript import *
from utils import *
from puns.utils import CreateSpecMDP, Eventually, Order, Globally
from puns.SpecificationMDP import *
from puns.LearningAgents import QLearningAgent
from puns.Exploration import ExplorerAgent
from numpy.random import binomial
import numpy as np
from scipy.stats import entropy
from matplotlib.backends.backend_pdf import PdfPages
import dill
import auto_eval_params as params
import os
import json



def run_paired_trials(ground_truth_formula = None):

    run_types = ['Active', 'Random', 'Batch']
    trial_functions = [run_active_query_trial, run_random_query_trial, run_batch_trial, run_baseline_trial]
    out_data = {}
    out_data['true_formulas'] = []
    for type in run_types:
        out_data[type] = {}
        out_data[type]['map_formulas'] = []
        out_data[type]['entropies'] = []
        out_data[type]['dists'] = []

    for i in range(params.n_runs):

        #Sample the ground truth formulas
        if ground_truth_formula == None:
            ground_truth_formula = sample_ground_truth()
        out_data['true_formulas'].append(ground_truth_formula)

        #Run the trial with the selected ground ground_truth_formula
        for (type, trial) in zip(run_types, trial_functions):
            print(f'Run {i} {type} trial')
            Distributions, MDPs, Queries, ground_truth_formula = trial(
            n_demo = params.n_demo, n_query = params.n_queries, run_id = i,
            ground_truth_formula = ground_truth_formula)

            out_data[type]['entropies'].append(entropy(Distributions[-1]['probs']))
            out_data[type]['map_formulas'].append(Distributions[-1]['formulas'][0])
            out_data[type]['dists'].append(Distributions[-1])

            create_run_log(i,type=type)
            del Distributions, MDPs, Queries

        for type in run_types:
            out_data[type]['average_entropy'] = np.mean(out_data[type]['entropies'])
        summary_file = os.path.join(params.results_path,'paired_summary.pkl')
        with open(summary_file,'wb') as file:
            dill.dump(out_data,file)
    return out_data

def run_trials(run_type='Active'):
    if run_type == 'Active':
        trial_function = run_active_query_trial
    elif run_type == 'Random':
        trial_function = run_random_query_trial
    else:
        trial_function = run_batch_trial
    #trial_function = run_active_query_trial

    final_distributions = []
    ground_truth_formulas = []
    final_map_formulas = []
    final_entropies = []
    for i in range(params.n_runs):

        Distributions, MDPs, Queries, ground_truth_formula = trial_function(
        n_demo = params.n_demo, n_query = params.n_queries, run_id = i)

        final_entropies.append(entropy(Distributions[-1]['probs']))
        final_distributions.append(Distributions[-1])
        ground_truth_formulas.append(ground_truth_formula)
        final_map_formulas.append(Distributions[-1]['formulas'][0])

        create_run_log(i, type=run_type)
        del Distributions, MDPs, Queries
    out_data = {}
    out_data['true_formulas'] = ground_truth_formulas
    out_data['map_formulas'] = final_map_formulas
    out_data['entropies'] = final_entropies
    out_data['dists'] = final_distributions
    out_data['average_entropy'] = np.mean(final_entropies)
    summary_file = os.path.join(params.results_path,f'{run_type}_summary.pkl')
    with open(summary_file,'wb') as file:
        dill.dump(out_data, file)
    return out_data

def run_batch_trial(n_demo = 2, n_query = 4, run_id = 1, ground_truth_formula = None, verbose = True, write_file=True):

        MDPs = [] #The list of all the MDPs compiled

        Distributions = []
        '''The list of the distributions computed inferred
        Each entry is a dictionary with keys 'formulas' and 'probs'
        can be used to compute the entropies
        '''
        Queries = []
        '''The list of the queries generated by the active query mechanism
        Each entry is a dictionary with keys 'trace', 'agent' and 'desired state'
        '''
        #Clear the data from previous runs
        clear_demonstrations(params)


        # Generate the demonstrations
        if ground_truth_formula == None:
            if verbose: print(f'Trial {run_id}: Sampling ground truth and generating demonstrations')
            ground_truth_formula = sample_ground_truth()
        create_demonstrations(ground_truth_formula, n_demo+n_query)

        # Run batch Inference
        infer_command = f'webppl batch_bsi.js --require webppl-json --require webppl-fs -- --nSamples {params.n_samples}  --nBurn {params.n_burn} --dataPath \'{params.compressed_data_path}\' --outPath \'{params.distributions_path}\' --nTraj {n_demo+n_query}'
        returnval = os.system(infer_command)
        if returnval: Exception('Inference Failure')

        # Compile the first MDP
        spec_file = os.path.join(params.distributions_path, 'batch_posterior.json')
        MDPs.append(CreateSpecMDP(spec_file, n_threats = 0, n_waypoints = params.n_waypoints))
        Distributions.append(extract_dist(MDPs[-1]))

        if write_file:
            write_run_data(Distributions, MDPs, Queries, ground_truth_formula, run_id, type='Batch')
        return Distributions, MDPs, Queries, ground_truth_formula

def run_random_query_trial(n_demo = 2, n_query = 4, run_id = 1, ground_truth_formula = None, verbose = True, write_file=True):

        MDPs = [] #The list of all the MDPs compiled

        Distributions = []
        '''The list of the distributions computed inferred
        Each entry is a dictionary with keys 'formulas' and 'probs'
        can be used to compute the entropies
        '''
        Queries = []
        '''The list of the queries generated by the active query mechanism
        Each entry is a dictionary with keys 'trace', 'agent' and 'desired state'
        '''
        #Clear the data from previous runs
        clear_demonstrations(params)


        # Generate the demonstrations
        if ground_truth_formula == None:
            if verbose: print(f'Trial {run_id}: Sampling ground truth and generating demonstrations')
            ground_truth_formula = sample_ground_truth()
        create_demonstrations(ground_truth_formula, n_demo)

        # Run batch Inference
        infer_command = f'webppl batch_bsi.js --require webppl-json --require webppl-fs -- --nSamples {params.n_samples}  --nBurn {params.n_burn} --dataPath \'{params.compressed_data_path}\' --outPath \'{params.distributions_path}\' --nTraj {n_demo}'
        returnval = os.system(infer_command)
        if returnval: Exception('Inference Failure')

        # Compile the first MDP
        spec_file = os.path.join(params.distributions_path, 'batch_posterior.json')
        MDPs.append(CreateSpecMDP(spec_file, n_threats = 0, n_waypoints = params.n_waypoints))
        Distributions.append(extract_dist(MDPs[-1]))

        #Start running the queries and active Inference
        for i in range(n_query):

            # Create the query
            if verbose: print(f'Trial {run_id}: Generating query {i+1} demo')
            Queries.append(create_random_query(MDPs[-1], verbose=verbose))

            # Eliciting feedback label for the query from ground ground_truth_formula
            signal = create_signal(Queries[-1]['trace'])
            label = Progress(ground_truth_formula, signal)[0]
            if verbose:
                print(f'Trial {run_id}: Generating ground truth label for query {i+1}')
                print('Assigned label', label)

            # Writing the query data to the inference database
            new_traj = create_query_demo(Queries[-1]['trace'])
            write_demo_query_data(new_traj, label, params.compressed_data_path, query_number = i+1)

            # Update the posterior distribution using active BSI
            if verbose: print(f'Trial {run_id}: Updating posterior after query {i+1}')
            infer_command = f'webppl active_bsi.js --require webppl-json --require webppl-fs -- --nSamples {params.n_samples}  --nBurn {params.n_burn} --dataPath \'{params.compressed_data_path}\' --outPath \'{params.distributions_path}\' --nQuery {i+1}'
            returnval = os.system(infer_command)
            if returnval: Exception('Inference failure')

            # Recompile the MDP with the updated specification and add the distributions
            spec_file = spec_file = os.path.join(params.distributions_path, 'batch_posterior.json')
            MDPs.append(CreateSpecMDP(spec_file, n_threats = 0, n_waypoints = params.n_waypoints))
            Distributions.append(extract_dist(MDPs[-1]))

        if write_file:
            write_run_data(Distributions, MDPs, Queries, ground_truth_formula, run_id, type='Random')
        return Distributions, MDPs, Queries, ground_truth_formula


def run_baseline_trial(n_demo = 2, n_query = 4, run_id = 1, ground_truth_formula = None, verbose=True, write_file=True):

    MDPs = [] #The list of all the MDPs compiled

    Distributions = []
    '''The list of the distributions computed inferred
    Each entry is a dictionary with keys 'formulas' and 'probs'
    can be used to compute the entropies
    '''
    Queries = []
    '''The list of the queries generated by the active query mechanism
    Each entry is a dictionary with keys 'trace', 'agent' and 'desired state'
    '''
    #Clear the data from previous runs
    clear_demonstrations(params)


    # Generate the demonstrations
    if ground_truth_formula == None:
        if verbose: print(f'Trial {run_id}: Sampling ground truth and generating demonstrations')
        ground_truth_formula = sample_ground_truth()
    create_demonstrations(ground_truth_formula, n_demo)

    # Run batch Inference
    infer_command = f'webppl batch_bsi.js --require webppl-json --require webppl-fs -- --nSamples {params.n_samples}  --nBurn {params.n_burn} --dataPath \'{params.compressed_data_path}\' --outPath \'{params.distributions_path}\' --nTraj {n_demo}'
    returnval = os.system(infer_command)
    if returnval: Exception('Inference Failure')

    # Compile the first MDP
    spec_file = os.path.join(params.distributions_path, 'batch_posterior.json')
    MDPs.append(CreateSpecMDP(spec_file, n_threats = 0, n_waypoints = params.n_waypoints))
    Distributions.append(extract_dist(MDPs[-1]))

    #Start running the queries and active Inference
    for i in range(n_query):

        # Create the query
        if verbose: print(f'Trial {run_id}: Generating query {i+1} demo')
        Queries.append(create_baseline_query(MDPs[-1], verbose=verbose))

        # Eliciting feedback label for the query from ground ground_truth_formula
        signal = create_signal(Queries[-1]['trace'])
        label = Progress(ground_truth_formula, signal)[0]
        if verbose:
            print(f'Trial {run_id}: Generating ground truth label for query {i+1}')
            print('Assigned label', label)

        # Writing the query data to the inference database
        new_traj = create_query_demo(Queries[-1]['trace'])
        write_demo_query_data(new_traj, label, params.compressed_data_path, query_number = i+1)

        # Update the posterior distribution using active BSI
        if verbose: print(f'Trial {run_id}: Updating posterior after query {i+1}')
        infer_command = f'webppl active_bsi.js --require webppl-json --require webppl-fs -- --nSamples {params.n_samples}  --nBurn {params.n_burn} --dataPath \'{params.compressed_data_path}\' --outPath \'{params.distributions_path}\' --nQuery {i+1}'
        returnval = os.system(infer_command)
        if returnval: Exception('Inference failure')

        # Recompile the MDP with the updated specification and add the distributions
        spec_file = spec_file = os.path.join(params.distributions_path, 'batch_posterior.json')
        MDPs.append(CreateSpecMDP(spec_file, n_threats = 0, n_waypoints = params.n_waypoints))
        Distributions.append(extract_dist(MDPs[-1]))

    if write_file:
        write_run_data(Distributions, MDPs, Queries, ground_truth_formula, run_id, type='Base')
    return Distributions, MDPs, Queries, ground_truth_formula


def run_active_query_trial(n_demo = 2, n_query = 4, run_id = 1, ground_truth_formula = None, verbose=True, write_file=True):

    MDPs = [] #The list of all the MDPs compiled

    Distributions = []
    '''The list of the distributions computed inferred
    Each entry is a dictionary with keys 'formulas' and 'probs'
    can be used to compute the entropies
    '''
    Queries = []
    '''The list of the queries generated by the active query mechanism
    Each entry is a dictionary with keys 'trace', 'agent' and 'desired state'
    '''
    #Clear the data from previous runs
    clear_demonstrations(params)


    # Generate the demonstrations
    if ground_truth_formula == None:
        if verbose: print(f'Trial {run_id}: Sampling ground truth and generating demonstrations')
        ground_truth_formula = sample_ground_truth()
    create_demonstrations(ground_truth_formula, n_demo)

    # Run batch Inference
    infer_command = f'webppl batch_bsi.js --require webppl-json --require webppl-fs -- --nSamples {params.n_samples}  --nBurn {params.n_burn} --dataPath \'{params.compressed_data_path}\' --outPath \'{params.distributions_path}\' --nTraj {n_demo}'
    returnval = os.system(infer_command)
    if returnval: Exception('Inference Failure')

    # Compile the first MDP
    spec_file = os.path.join(params.distributions_path, 'batch_posterior.json')
    MDPs.append(CreateSpecMDP(spec_file, n_threats = 0, n_waypoints = params.n_waypoints))
    Distributions.append(extract_dist(MDPs[-1]))

    #Start running the queries and active Inference
    for i in range(n_query):

        # Create the query
        if verbose: print(f'Trial {run_id}: Generating query {i+1} demo')
        Queries.append(create_active_query(MDPs[-1], verbose=verbose, non_terminal = params.non_terminal))

        # Eliciting feedback label for the query from ground ground_truth_formula
        signal = create_signal(Queries[-1]['trace'])
        label = Progress(ground_truth_formula, signal)[0]
        if verbose:
            print(f'Trial {run_id}: Generating ground truth label for query {i+1}')
            print('Assigned label', label)

        # Writing the query data to the inference database
        new_traj = create_query_demo(Queries[-1]['trace'])
        write_demo_query_data(new_traj, label, params.compressed_data_path, query_number = i+1)

        # Update the posterior distribution using active BSI
        if verbose: print(f'Trial {run_id}: Updating posterior after query {i+1}')
        infer_command = f'webppl active_bsi.js --require webppl-json --require webppl-fs -- --nSamples {params.n_samples}  --nBurn {params.n_burn} --dataPath \'{params.compressed_data_path}\' --outPath \'{params.distributions_path}\' --nQuery {i+1}'
        returnval = os.system(infer_command)
        if returnval: Exception('Inference failure')

        # Recompile the MDP with the updated specification and add the distributions
        spec_file = spec_file = os.path.join(params.distributions_path, 'batch_posterior.json')
        MDPs.append(CreateSpecMDP(spec_file, n_threats = 0, n_waypoints = params.n_waypoints))
        Distributions.append(extract_dist(MDPs[-1]))

    if write_file:
        write_run_data(Distributions, MDPs, Queries, ground_truth_formula, run_id, type='Active')
    return Distributions, MDPs, Queries, ground_truth_formula

def write_run_data(Distributions, MDPs, Queries, ground_truth_formula, run_id, type = 'Active'):
    if not os.path.exists(os.path.join(params.results_path,'Runs')): os.mkdir(os.path.join(params.results_path,'Runs'))
    filename = os.path.join(params.results_path, 'Runs', f'{type}_Run_{run_id}.pkl')
    data = {}
    data['Distributions'] = Distributions
    data['MDPs'] = MDPs
    data['Queries'] = [q['trace'] for q in Queries]
    data['True_Formula'] = ground_truth_formula

    with open(filename,'wb') as file:
        dill.dump(data, file)

def create_run_log(run_id, type = 'Active'):
    filename = os.path.join(params.results_path, 'Runs', f'{type}_Run_{run_id}')
    with open(filename+'.pkl','rb') as file:
        data = dill.load(file)

    if type == 'Batch':
        n_demo = params.n_demo + params.n_queries
    else:
        n_demo = params.n_demo

    pdf_file = filename+'.pdf'
    with PdfPages(pdf_file) as pdf:
        #print the ground truth formula
        plt.figure()
        true_form = json.dumps(data['True_Formula'])
        plt.text(0,0,true_form, wrap = True)
        plt.axis([-0.1,1,-1,1])
        plt.title(f'Trial {run_id}: Ground truth')
        pdf.savefig()
        plt.close()

        # Plot the original batch distributions
        plt.figure(figsize = (5,5))
        probs = data['Distributions'][0]['probs']
        plt.bar(list(range(len(probs))), probs)
        plt.title(f'Trial {run_id}: Posterior after {n_demo} demonstrations')
        pdf.savefig()
        plt.close()

        for i in range(len(data['Queries'])):
            #plot the query_
            plt.figure(figsize=(5,5))
            visualize_query(data['Queries'][i])
            plt.title(f'Query {i+1}')
            pdf.savefig()
            plt.close()

            #plot the distribution after the query_
            plt.figure(figsize = (5,5))
            probs = data['Distributions'][i+1]['probs']
            plt.bar(list(range(len(probs))), probs)
            plt.title(f'Posterior after query {i+1}')
            pdf.savefig()
            plt.close()

        # Print the MAP
        plt.figure()
        true_form = json.dumps(data['Distributions'][-1]['formulas'][0])
        plt.text(0,0,true_form, wrap=True)
        entropyval = entropy(data['Distributions'][-1]['probs'])
        plt.text(0, -0.9, f'Entropy: {entropyval}')
        plt.axis([-0.1,1,-1,1])
        plt.title(f'Trial {run_id}: MAP')
        pdf.savefig()
        plt.close()





def extract_dist(MDP:SpecificationMDP):
    new_dist = {}
    new_dist['formulas'] = MDP.specification_fsm._all_formulas
    new_dist['probs'] = MDP.specification_fsm._all_probs
    return new_dist

def create_demonstrations(formula, nDemo, verbose = True, n_threats = 0):

    specification_fsm = SpecificationFSM(formulas=[formula], probs = [1])
    control_mdp = SyntheticMDP(0,params.n_waypoints)
    MDP = SpecificationMDP(specification_fsm, control_mdp)

    q_agent = QLearningAgent(MDP)
    print('Training ground truth demonstrator')
    q_agent.explore(episode_limit = 5000, verbose=verbose, action_limit = 1000000)
    eval_agent = ExplorerAgent(MDP, input_policy=q_agent.create_learned_softmax_policy(0.005))
    print('\n')
    eval_agent.explore(episode_limit = nDemo)

    for record in eval_agent.episodic_record:

        trace_slices = [MDP.control_mdp.create_observations(rec[0][1]) for rec in record]
        trace_slices.append(MDP.control_mdp.create_observations(record[-1][2][1]))

        new_traj = create_query_demo(trace_slices)

        write_demo_query_data(new_traj, True, params.compressed_data_path, filename='demo')
    return eval_agent

def waypoints_and_orders(formula):
    #assume that formula is in ['and',....] format
    waypoints = []
    orders = []
    threats = []

    if formula[0] == 'and':
        subformulas = formula[1::]
    else:
        subformulas = [formula]

    for sub_formula in subformulas:
        if sub_formula[0] == 'F':
            waypoints.append(sub_formula[1][0])
        elif sub_formula[0]=='U':
            w1 = sub_formula[2][0]
            w2 = sub_formula[1][1][0]
            orders.append((w1,w2))
            waypoints.append(w1)
        elif sub_formula[0] == 'G':
            threats.append('G' + sub_formula[1][1][0])
        else:
            waypoints.append(0)

        #Remove the orders whose precedents are in Globals

        for order in orders:
            if 'G' + order[1] in threats:
                orders.remove(order)

    return waypoints, orders, threats

def compare_formulas(formula_1, formula_2):

    # Assume that they are in ['and' ...] format
    waypoints1, orders1, globals1 = waypoints_and_orders(formula_1)
    waypoints2, orders2, globals2 = waypoints_and_orders(formula_2)

    clauses_1 = set(waypoints1 + orders1 + globals1)
    clauses_2 = set(waypoints2 + orders2 + globals2)
    try:
        L = len(set.intersection(clauses_1,clauses_2))/len(set.union(clauses_1,clauses_2))
    except:
           print(clauses_1, clauses_2)
           print(formula_1, formula_2)
           L = 0
    return L

def compare_distribution(true_formula, distribution):
    similarities = [compare_formulas(true_formula, form) for form in distribution['formulas']]
    return np.dot(similarities, distribution['probs'])

def report_entropies(typ = None):
    if typ == None:
        types = ['Active','Batch','Random', 'Base']
    elif type(typ) == list:
        types = typ
    else:
        types = [typ]


    filename = os.path.join(params.results_path, 'paired_summary.pkl')
    with open(filename, 'rb') as file:
        data = dill.load(file)

    Entropies = {}
    Entropies['individual'] = {}
    Entropies['average'] = {}

    for typ in types:
        Entropies['individual'][typ] = data[typ]['entropies']
        Entropies['average'][typ] = np.mean(data[typ]['entropies'])

    Entropies = pd.DataFrame(Entropies['individual'])
    return Entropies

def report_map_similarities(typ=None):
    if typ == None:
        types = ['Active','Batch','Random', 'Base']
    elif type(typ) == list:
        types = typ
    else:
        types = [typ]

    similarities = {}
    similarities['individual'] = {}
    similarities['average'] = {}

    filename = os.path.join(params.results_path, 'paired_summary.pkl')
    with open(filename, 'rb') as file:
        data = dill.load(file)

    for typ in types:
        similarities['individual'][typ] = [compare_formulas(f1, f2) for (f1,f2)
                        in zip(data['true_formulas'], data[typ]['map_formulas'])]
        similarities['average'][typ] = np.mean(similarities['individual'][typ])

    similarities = pd.DataFrame(similarities['individual'])

    return similarities

def report_similarities(typ=None):
    if typ == None:
        types = ['Active','Batch','Random', 'Base']
    elif type(typ) == list:
        types = typ
    else:
        types = [typ]

    similarities = {}
    similarities['individual'] = {}
    similarities['average'] = {}

    filename = os.path.join(params.results_path, 'paired_summary.pkl')
    with open(filename, 'rb') as file:
        data = dill.load(file)

    for typ in types:
        similarities['individual'][typ] = [compare_distribution(f1,d) for
                    (f1,d) in zip(data['true_formulas'], data[typ]['dists'])]
        similarities['average'][typ] = np.mean(similarities['individual'][typ])

    similarities = pd.DataFrame(similarities['individual'])
    return similarities

def assimilate_metrics(typ):
    Entropy = report_entropies(typ)
    Similarity = report_similarities(typ)

    data = {}
    for i in Entropy.index:
        for column in   Entropy.columns:
            ids = len(data)
            data[ids] = {}
            data[ids]['Entropy'] = Entropy.loc[i, column]
            data[ids]['Similarity'] = Similarity.loc[i, column]
            data[ids]['type'] = column
            data[ids]['n_data'] = params.n_queries + params.n_demo

    data = pd.DataFrame.from_dict(data, orient='index')
    data.reset_index()
    return data


def get_summary():
    with open(os.path.join(params.results_path, 'paired_summary.pkl'),'rb') as file:
        data = dill.load(file)
    return data

def get_run_data(i=0, run_type = 'Active'):
    filename = os.path.join(params.results_path, 'Runs',f'{run_type}_Run_{i}.pkl')
    with open(filename, 'rb') as file:
        run_data = dill.load(file)
    return run_data

def create_results_path():
    if not os.path.exists(params.results_path):
        os.mkdir(params.results_path)
        os.mkdir(os.path.join(params.results_path, 'Runs'))

if __name__ == '__main__':

    for i in [3]:
        params.n_queries = i
        print(f'Running evaluations for {params.n_demo} demonstrations and {params.n_queries} queries')
        n = i + params.n_demo
        params.results_path = f'/home/ajshah/Results/TableSetup_Task2_{n}_with_baseline'
        create_results_path()
        params.n_runs = 20
        # ground_truth = ['and']
        # for i in range(5):
        #     ground_truth.append(Eventually(f'W{i}'))
        # ground_truth.append(Order('W0','W1'))
        # ground_truth.append(Order('W0','W2'))
        # ground_truth.append(Order('W1','W2'))

        ground_truth = ['and']
        ground_truth.append(Globally('W0'))
        ground_truth.append(Globally('W2'))
        ground_truth.append(Eventually('W1'))
        #ground_truth.append(Eventually('W3'))
        ground_truth.append(Eventually('W4'))

        out_data = run_paired_trials(ground_truth)

        '''task 3'''
        params.n_queries = i
        print(f'Running evaluations for {params.n_demo} demonstrations and {params.n_queries} queries')
        n = i + params.n_demo
        params.results_path = f'/home/ajshah/Results/TableSetup_Task3_{n}_with_baseline'
        create_results_path()
        params.n_runs = 20
        # ground_truth = ['and']
        # for i in range(5):
        #     ground_truth.append(Eventually(f'W{i}'))
        # ground_truth.append(Order('W0','W1'))
        # ground_truth.append(Order('W0','W2'))
        # ground_truth.append(Order('W1','W2'))

        ground_truth = ['and']
        ground_truth.append(Globally('W0'))
        ground_truth.append(Globally('W2'))
        ground_truth.append(Eventually('W1'))
        #ground_truth.append(Eventually('W2'))
        #ground_truth.append(Order('W1','W2'))
        #ground_truth.append(Eventually('W3'))
        #ground_truth.append(Eventually('W4'))

        out_data = run_paired_trials(ground_truth)

        '''task 4'''
        params.n_queries = i
        print(f'Running evaluations for {params.n_demo} demonstrations and {params.n_queries} queries')
        n = i + params.n_demo
        params.results_path = f'/home/ajshah/Results/TableSetup_Task4_{n}_with_baseline'
        create_results_path()
        params.n_runs = 20
        # ground_truth = ['and']
        # for i in range(5):
        #     ground_truth.append(Eventually(f'W{i}'))
        # ground_truth.append(Order('W0','W1'))
        # ground_truth.append(Order('W0','W2'))
        # ground_truth.append(Order('W1','W2'))

        ground_truth = ['and']
        ground_truth.append(Globally('W0'))
        #ground_truth.append(Globally('W2'))
        ground_truth.append(Eventually('W1'))
        ground_truth.append(Eventually('W4'))
        ground_truth.append(Order('W1','W2'))
        #ground_truth.append(Eventually('W2'))
        #ground_truth.append(Order('W1','W2'))
        #ground_truth.append(Eventually('W3'))
        #ground_truth.append(Eventually('W4'))

        out_data = run_paired_trials(ground_truth)

        '''task 5'''
        params.n_queries = i
        print(f'Running evaluations for {params.n_demo} demonstrations and {params.n_queries} queries')
        n = i + params.n_demo
        params.results_path = f'/home/ajshah/Results/TableSetup_Task5_{n}_with_baseline'
        create_results_path()
        params.n_runs = 20
        # ground_truth = ['and']
        # for i in range(5):
        #     ground_truth.append(Eventually(f'W{i}'))
        # ground_truth.append(Order('W0','W1'))
        # ground_truth.append(Order('W0','W2'))
        # ground_truth.append(Order('W1','W2'))

        ground_truth = ['and']
        ground_truth.append(Globally('W0'))
        #ground_truth.append(Globally('W2'))
        ground_truth.append(Eventually('W1'))
        ground_truth.append(Eventually('W2'))
        ground_truth.append(Order('W1','W2'))
        #ground_truth.append(Eventually('W2'))
        #ground_truth.append(Order('W1','W2'))
        #ground_truth.append(Eventually('W3'))
        #ground_truth.append(Eventually('W4'))

        out_data = run_paired_trials(ground_truth)
